# Features V2 å¢é‡å¼ç‰¹å¾å·¥ç¨‹å®æ–½æ–¹æ¡ˆ

## æ ¸å¿ƒè®¾è®¡å†³ç­–

### é—®é¢˜3ï¼šé…ç½®åˆå¹¶ç­–ç•¥ - "æ™ºèƒ½åˆå¹¶ï¼Œä¸ä¸¢ç‰¹å¾"

**ç­–ç•¥**ï¼š
```python
# append=True æ—¶çš„åˆå¹¶é€»è¾‘
if feature_exists_in_old_config:
    if definition_is_same:
        # ä¿ç•™ï¼Œè·³è¿‡
        logger.info(f"ç‰¹å¾ {name} å·²å­˜åœ¨ä¸”å®šä¹‰ç›¸åŒï¼Œè·³è¿‡")
    else:
        # å®šä¹‰ä¸åŒï¼Œåˆ›å»ºæ–°ç‰ˆæœ¬
        new_name = f"{name}_v{version}"
        logger.warning(f"ç‰¹å¾ {name} å®šä¹‰å†²çªï¼Œä¿å­˜ä¸º {new_name}")
else:
    # æ–°ç‰¹å¾ï¼Œç›´æ¥è¿½åŠ 
    logger.info(f"æ·»åŠ æ–°ç‰¹å¾: {name}")
```

**Parquet åˆå¹¶**ï¼š
- åŒåç‰¹å¾ï¼šè¦†ç›–ï¼ˆä½¿ç”¨æ–°è®¡ç®—ç»“æœï¼‰
- æ–°ç‰¹å¾ï¼šè¿½åŠ åˆ—
- ä¿è¯ `step_index` å¯¹é½

---

### é—®é¢˜4ï¼šç¼“å­˜å¤±æ•ˆç­–ç•¥ - "è½»é‡çº§å“ˆå¸ŒéªŒè¯"

**æ–¹æ¡ˆ**ï¼šåŸºäºå…ƒæ•°æ®å“ˆå¸Œï¼ˆæœ€ä½³å¹³è¡¡ç‚¹ï¼‰

#### ç¼“å­˜é”®è®¾è®¡
```python
source_hash = md5(f"{chip_id}|{device_id}|{hdf5_created_at}|{file_size}")
# ä¼˜ç‚¹ï¼š
# - ä¸éœ€è¦è¯»å– HDF5 å…¨éƒ¨å†…å®¹
# - HDF5 é‡æ–°ç”Ÿæˆå created_at ä¼šå˜
# - file_size ä½œä¸ºé¢å¤–æ ¡éªŒ
```

#### Parquet å…ƒæ•°æ®æ‰©å±•
```python
# ä¿å­˜æ—¶è‡ªåŠ¨å†™å…¥
df.attrs = {
    'chip_id': '#20250804008',
    'device_id': '3',
    'config_name': 'my_base_features',
    'config_version': '1.2',  # æ¯æ¬¡ append é€’å¢
    'source_file': '/path/to/test_xxx.h5',
    'source_hash': 'abc123...',  # è½»é‡çº§å“ˆå¸Œ
    'created_at': '2025-10-31T15:00:00',
    'feature_count': 5
}
```

#### è‡ªåŠ¨éªŒè¯é€»è¾‘
```python
def compute(self):
    cached_df = self.unified_experiment.get_v2_feature_dataframe(self.config_name)
    
    if cached_df is not None:
        # éªŒè¯ç¼“å­˜æœ‰æ•ˆæ€§
        if self._validate_cache(cached_df):
            logger.info("âœ“ ç¼“å­˜æœ‰æ•ˆï¼Œä½¿ç”¨ Parquet æ•°æ®")
            return self._load_from_cache(cached_df)
        else:
            logger.warning("âš  ç¼“å­˜å¤±æ•ˆï¼ˆæºæ–‡ä»¶å·²æ”¹å˜ï¼‰ï¼Œé‡æ–°è®¡ç®—")
            # ç»§ç»­æ­£å¸¸è®¡ç®—æµç¨‹
    
    # æ­£å¸¸è®¡ç®—...

def _validate_cache(self, cached_df):
    """éªŒè¯ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
    metadata = cached_df.attrs
    
    # è®¡ç®—å½“å‰æºæ–‡ä»¶å“ˆå¸Œ
    current_hash = self._compute_source_hash()
    cached_hash = metadata.get('source_hash')
    
    if current_hash != cached_hash:
        return False  # æºæ–‡ä»¶æ”¹å˜
    
    return True

def _compute_source_hash(self):
    """è®¡ç®—æºæ–‡ä»¶è½»é‡çº§å“ˆå¸Œ"""
    exp = self.unified_experiment._get_experiment()
    file_path = exp.hdf5_path
    stat = Path(file_path).stat()
    
    # ç»„åˆå¤šä¸ªå…ƒæ•°æ®ï¼ˆä¸è¯»å–æ–‡ä»¶å†…å®¹ï¼‰
    hash_input = f"{self.unified_experiment.chip_id}|{self.unified_experiment.device_id}|{stat.st_mtime}|{stat.st_size}"
    return hashlib.md5(hash_input.encode()).hexdigest()
```

**ç¼“å­˜å¤±æ•ˆåœºæ™¯å¤„ç†**ï¼š
| åœºæ™¯ | æ£€æµ‹æ–¹æ³• | è¡Œä¸º |
|------|----------|------|
| HDF5 é‡æ–°ç”Ÿæˆ | `st_mtime` æ”¹å˜ | è‡ªåŠ¨é‡æ–°è®¡ç®— âœ… |
| HDF5 è¢«å¤åˆ¶/ç§»åŠ¨ | `st_mtime` ä¿ç•™ | ä»ç„¶æœ‰æ•ˆï¼ˆå†…å®¹æœªå˜ï¼‰ âœ… |
| æå–å™¨ä»£ç æ”¹å˜ | Phase 2 æ‰©å±•ï¼ˆç‰ˆæœ¬å·ï¼‰ | å½“å‰ï¼šç”¨æˆ·æ‰‹åŠ¨ `force_recompute` |
| é…ç½®å‚æ•°æ”¹å˜ | é…ç½®åä¸åŒ â†’ æ–°ç¼“å­˜æ–‡ä»¶ | è‡ªåŠ¨å¤„ç† âœ… |

---

## ğŸ“‹ å®Œæ•´å®æ–½æ–¹æ¡ˆ

### æ¨¡å— 1ï¼šFeatureSet æ ¸å¿ƒæ‰©å±•

#### 1.1 æ„é€ å‡½æ•°æ‰©å±•
**æ–‡ä»¶**ï¼š`infra/features_v2/core/feature_set.py`

```python
def __init__(
    self, 
    experiment=None, 
    unified_experiment=None,  # æ–°å¢
    config_name=None,         # æ–°å¢
    config_version='1.0'      # æ–°å¢
):
    """
    Args:
        experiment: åº•å±‚ Experiment å¯¹è±¡
        unified_experiment: UnifiedExperiment å¯¹è±¡ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
        config_name: é…ç½®åç§°ï¼ˆç”¨äºç¼“å­˜æŸ¥æ‰¾ï¼‰
        config_version: é…ç½®ç‰ˆæœ¬å·
    """
    self.unified_experiment = unified_experiment
    self.config_name = config_name
    self.config_version = config_version
    
    # è‡ªåŠ¨æå–åº•å±‚ experiment
    if unified_experiment and not experiment:
        experiment = unified_experiment._get_experiment()
    
    self.experiment = experiment
    self.graph = ComputeGraph()
    self.data_loaders = {}
    self._computed_results: Optional[ExecutionContext] = None
    
    if experiment:
        self._setup_data_loaders()
```

#### 1.2 from_config æ‰©å±•
```python
@classmethod
def from_config(
    cls, 
    config_path: str, 
    experiment=None, 
    unified_experiment=None
):
    """ä»é…ç½®æ–‡ä»¶åŠ è½½ï¼ˆè‡ªåŠ¨æå–é…ç½®åç§°ï¼‰"""
    from pathlib import Path
    from infra.features_v2.config.parser import ConfigParser
    
    # æå–é…ç½®åç§°
    config_name = Path(config_path).stem
    
    # è§£æé…ç½®
    if unified_experiment:
        experiment = unified_experiment._get_experiment()
    
    parsed_config = ConfigParser.from_file(config_path, experiment)
    
    # è¯»å–ç‰ˆæœ¬å·ï¼ˆä»é…ç½®æ–‡ä»¶ï¼‰
    config_version = parsed_config.config.version if hasattr(parsed_config.config, 'version') else '1.0'
    
    # è®¾ç½®å…ƒæ•°æ®
    parsed_config.unified_experiment = unified_experiment
    parsed_config.config_name = config_name
    parsed_config.config_version = config_version
    
    return parsed_config
```

#### 1.3 å¢é‡è®¡ç®—ï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰
```python
def compute(self) -> Dict[str, np.ndarray]:
    """å¢é‡è®¡ç®—ï¼šä¼˜å…ˆä» Parquet åŠ è½½å·²æœ‰ç‰¹å¾"""
    
    # 1ï¸âƒ£ å°è¯•åŠ è½½ç¼“å­˜
    cached_features = {}
    if self.unified_experiment and self.config_name:
        cached_df = self.unified_experiment.get_v2_feature_dataframe(self.config_name)
        
        if cached_df is not None:
            # éªŒè¯ç¼“å­˜æœ‰æ•ˆæ€§
            if self._validate_cache(cached_df):
                logger.info(f"âœ“ å‘ç°æœ‰æ•ˆç¼“å­˜ï¼ˆé…ç½®: {self.config_name}ï¼‰")
                
                # æå–æ‰€æœ‰ç¼“å­˜ç‰¹å¾
                for col in cached_df.columns:
                    if col != 'step_index' and col in self.graph.nodes:
                        cached_features[col] = cached_df[col].to_numpy()
                        logger.info(f"  âœ“ ä»ç¼“å­˜åŠ è½½: {col}")
            else:
                logger.warning(f"âš  ç¼“å­˜å¤±æ•ˆï¼Œé‡æ–°è®¡ç®—")
    
    # 2ï¸âƒ£ æ£€æŸ¥æ˜¯å¦å…¨éƒ¨å‘½ä¸­ç¼“å­˜
    all_features = set(self.graph.nodes.keys())
    cached_feature_names = set(cached_features.keys())
    missing_features = all_features - cached_feature_names
    
    if not missing_features:
        # å…¨éƒ¨å‘½ä¸­ç¼“å­˜
        logger.info(f"âœ“ å…¨éƒ¨ {len(cached_features)} ä¸ªç‰¹å¾ä»ç¼“å­˜åŠ è½½ï¼Œæ— éœ€è®¡ç®—")
        
        # å¡«å…… ExecutionContext
        self._computed_results = ExecutionContext()
        for name, value in cached_features.items():
            self._computed_results.set(name, value, 0)
        
        return cached_features
    
    # 3ï¸âƒ£ éƒ¨åˆ†å‘½ä¸­ï¼šå¢é‡è®¡ç®—
    logger.info(
        f"âš™ï¸ å¢é‡è®¡ç®—ï¼š{len(cached_features)} ä¸ªä»ç¼“å­˜ï¼Œ"
        f"{len(missing_features)} ä¸ªéœ€è®¡ç®—"
    )
    
    # åˆ›å»ºåˆå§‹ä¸Šä¸‹æ–‡ï¼ˆé¢„å¡«å……ç¼“å­˜ç‰¹å¾ï¼‰
    initial_context = ExecutionContext()
    for name, value in cached_features.items():
        initial_context.set(name, value, 0)
    
    # å®ä¾‹åŒ–æå–å™¨ï¼ˆåªéœ€è¦æœªç¼“å­˜çš„ï¼‰
    extractor_instances = {}
    for node_name in missing_features:
        node = self.graph.nodes[node_name]
        if node.is_extractor:
            extractor_instances[node.func] = get_extractor(node.func, node.params)
    
    # æ‰§è¡Œè®¡ç®—ï¼ˆä¼ å…¥åˆå§‹ä¸Šä¸‹æ–‡ï¼‰
    executor = Executor(
        compute_graph=self.graph,
        data_loaders=self.data_loaders,
        extractor_registry=extractor_instances,
    )
    
    context = executor.execute(initial_context=initial_context)
    self._computed_results = context
    
    # 4ï¸âƒ£ è¿”å›æ‰€æœ‰ç‰¹å¾
    features = {}
    for name in self.graph.nodes:
        if name in context.results:
            features[name] = context.results[name]
    
    logger.info(
        f"âœ… è®¡ç®—å®Œæˆï¼š{len(features)} ä¸ªç‰¹å¾ï¼Œ"
        f"è€—æ—¶ {context.get_total_time():.2f}ms"
    )
    
    return features

def _validate_cache(self, cached_df: pd.DataFrame) -> bool:
    """éªŒè¯ Parquet ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
    if not hasattr(cached_df, 'attrs'):
        return False
    
    metadata = cached_df.attrs
    cached_hash = metadata.get('source_hash')
    
    if not cached_hash:
        logger.warning("ç¼“å­˜ç¼ºå°‘ source_hashï¼Œæ— æ³•éªŒè¯")
        return True  # å‘åå…¼å®¹æ—§ç¼“å­˜
    
    # è®¡ç®—å½“å‰å“ˆå¸Œ
    current_hash = self._compute_source_hash()
    
    if current_hash != cached_hash:
        logger.debug(f"æºæ–‡ä»¶å“ˆå¸Œä¸åŒ¹é…: {current_hash} != {cached_hash}")
        return False
    
    return True

def _compute_source_hash(self) -> str:
    """è®¡ç®—æºæ–‡ä»¶è½»é‡çº§å“ˆå¸Œ"""
    import hashlib
    from pathlib import Path
    
    if not self.unified_experiment:
        return ""
    
    exp = self.unified_experiment._get_experiment()
    file_path = Path(exp.hdf5_path)
    
    if not file_path.exists():
        return ""
    
    stat = file_path.stat()
    hash_input = (
        f"{self.unified_experiment.chip_id}|"
        f"{self.unified_experiment.device_id}|"
        f"{stat.st_mtime}|"
        f"{stat.st_size}"
    )
    
    return hashlib.md5(hash_input.encode()).hexdigest()[:16]  # å‰16ä½è¶³å¤Ÿ
```

#### 1.4 é…ç½®å›ºåŒ–
```python
def save_as_config(
    self,
    config_name: str,
    save_parquet: bool = True,
    append: bool = False,
    config_dir: str = 'user',  # 'user' æˆ– 'global'
    description: str = ""
) -> Dict[str, str]:
    """
    å›ºåŒ–å½“å‰ç‰¹å¾é›†ä¸ºé…ç½® + Parquet
    
    Args:
        config_name: é…ç½®åç§°
        save_parquet: æ˜¯å¦ä¿å­˜ Parquet æ•°æ®
        append: æ˜¯å¦å¢é‡è¿½åŠ ï¼ˆåˆå¹¶å·²æœ‰é…ç½®ï¼‰
        config_dir: é…ç½®ä¿å­˜ä½ç½®
            - 'user': ~/.my_features/ ï¼ˆä¸ªäººé…ç½®ï¼‰
            - 'global': infra/catalog/feature_configs/ ï¼ˆå…¨å±€å…±äº«ï¼‰
        description: é…ç½®æè¿°
    
    Returns:
        {'config_file': '...', 'parquet_file': '...', 'features_added': [...]}
    """
    if not self._computed_results:
        raise RuntimeError("è¯·å…ˆè°ƒç”¨ compute() è®¡ç®—ç‰¹å¾")
    
    from pathlib import Path
    import yaml
    
    # 1ï¸âƒ£ ç¡®å®šä¿å­˜è·¯å¾„
    if config_dir == 'user':
        base_dir = Path.home() / '.my_features'
    elif config_dir == 'global':
        base_dir = Path(__file__).parent.parent.parent / 'catalog' / 'feature_configs'
    else:
        base_dir = Path(config_dir)
    
    base_dir.mkdir(parents=True, exist_ok=True)
    config_file = base_dir / f"{config_name}.yaml"
    
    # 2ï¸âƒ£ æ„å»ºé…ç½®å­—å…¸
    feature_specs = []
    for node_name, node in self.graph.nodes.items():
        spec = {
            'name': node_name,
            'input': node.inputs[0] if len(node.inputs) == 1 else node.inputs,
            'params': node.params if node.params else None
        }
        
        if node.is_extractor:
            spec['extractor'] = node.func
        else:
            # Lambda å‡½æ•°åºåˆ—åŒ–ä¸ºå­—ç¬¦ä¸²ï¼ˆç®€åŒ–ç‰ˆï¼‰
            import inspect
            spec['func'] = inspect.getsource(node.func).strip()
            spec['output_shape'] = list(node.output_shape) if node.output_shape else None
        
        feature_specs.append(spec)
    
    config_dict = {
        'version': self.config_version,
        'name': config_name,
        'description': description or f"Auto-generated config for {config_name}",
        'data_sources': [
            {'name': 'transfer', 'type': 'transfer'},
            {'name': 'transient', 'type': 'transient'}
        ],
        'features': feature_specs
    }
    
    # 3ï¸âƒ£ å¤„ç† append æ¨¡å¼
    features_added = []
    if append and config_file.exists():
        with open(config_file, 'r') as f:
            existing_config = yaml.safe_load(f)
        
        # åˆå¹¶ç‰¹å¾ï¼ˆæ™ºèƒ½å»é‡ï¼‰
        existing_features = {f['name']: f for f in existing_config.get('features', [])}
        
        for spec in feature_specs:
            name = spec['name']
            if name in existing_features:
                # æ£€æŸ¥å®šä¹‰æ˜¯å¦ç›¸åŒ
                if existing_features[name] == spec:
                    logger.info(f"ç‰¹å¾ {name} å·²å­˜åœ¨ä¸”å®šä¹‰ç›¸åŒï¼Œè·³è¿‡")
                else:
                    # å®šä¹‰å†²çªï¼Œä¿ç•™æ–°ç‰ˆæœ¬
                    logger.warning(f"ç‰¹å¾ {name} å®šä¹‰å·²æ›´æ–°")
                    existing_features[name] = spec
                    features_added.append(name)
            else:
                # æ–°ç‰¹å¾
                existing_features[name] = spec
                features_added.append(name)
        
        config_dict['features'] = list(existing_features.values())
        
        # é€’å¢ç‰ˆæœ¬å·
        old_version = existing_config.get('version', '1.0')
        major, minor = map(int, old_version.split('.'))
        config_dict['version'] = f"{major}.{minor + 1}"
        
        logger.info(f"âœ“ é…ç½®ç‰ˆæœ¬æ›´æ–°: {old_version} â†’ {config_dict['version']}")
    else:
        features_added = [spec['name'] for spec in feature_specs]
    
    # 4ï¸âƒ£ ä¿å­˜é…ç½®æ–‡ä»¶
    with open(config_file, 'w') as f:
        yaml.dump(config_dict, f, default_flow_style=False, allow_unicode=True)
    
    logger.info(f"âœ“ é…ç½®å·²ä¿å­˜: {config_file}")
    
    # 5ï¸âƒ£ ä¿å­˜ Parquetï¼ˆå¯é€‰ï¼‰
    parquet_file = None
    if save_parquet and self.unified_experiment:
        # ä½¿ç”¨ unified_experiment çš„æ–¹æ³•ï¼ˆè‡ªåŠ¨æ›´æ–°æ•°æ®åº“ï¼‰
        parquet_file = self.unified_experiment.extract_features_v2(
            feature_config=config_dict,
            output_format='parquet',
            force_recompute=True  # å¼ºåˆ¶ä¿å­˜å½“å‰è®¡ç®—ç»“æœ
        )
        logger.info(f"âœ“ Parquet å·²ä¿å­˜: {parquet_file}")
    
    return {
        'config_file': str(config_file),
        'parquet_file': parquet_file,
        'features_added': features_added,
        'config_version': config_dict['version']
    }
```

#### 1.5 Parquet å¢é‡åˆå¹¶
```python
def to_parquet(
    self, 
    output_path: str, 
    merge_existing: bool = False,
    save_metadata: bool = True
):
    """
    å¯¼å‡ºä¸º Parquetï¼Œå¯é€‰æ‹©å¢é‡åˆå¹¶
    
    Args:
        output_path: è¾“å‡ºè·¯å¾„
        merge_existing: æ˜¯å¦åˆå¹¶å·²æœ‰æ–‡ä»¶
        save_metadata: æ˜¯å¦ä¿å­˜å…ƒæ•°æ®ï¼ˆç”¨äºç¼“å­˜éªŒè¯ï¼‰
    """
    from pathlib import Path
    import pandas as pd
    
    output_path = Path(output_path)
    new_df = self.to_dataframe(expand_multidim=True)
    
    # å¢é‡åˆå¹¶
    if merge_existing and output_path.exists():
        logger.info(f"ğŸ”„ å¢é‡åˆå¹¶åˆ°å·²æœ‰æ–‡ä»¶: {output_path.name}")
        existing_df = pd.read_parquet(output_path)
        
        # ä¿ç•™æ—§å…ƒæ•°æ®ï¼ˆç¨åæ›´æ–°ï¼‰
        old_attrs = existing_df.attrs.copy() if hasattr(existing_df, 'attrs') else {}
        
        # åˆå¹¶åˆ—ï¼ˆè¦†ç›–åŒåï¼Œè¿½åŠ æ–°åˆ—ï¼‰
        for col in new_df.columns:
            if col != 'step_index':
                existing_df[col] = new_df[col]
        
        final_df = existing_df
        
        # æ›´æ–°ç‰¹å¾è®¡æ•°
        if save_metadata:
            old_attrs['feature_count'] = len(final_df.columns) - 1
            old_attrs['updated_at'] = pd.Timestamp.now().isoformat()
            final_df.attrs = old_attrs
    else:
        final_df = new_df
    
    # æ·»åŠ å…ƒæ•°æ®
    if save_metadata and self.unified_experiment:
        final_df.attrs = {
            'chip_id': self.unified_experiment.chip_id,
            'device_id': self.unified_experiment.device_id,
            'config_name': self.config_name or 'unknown',
            'config_version': self.config_version,
            'source_file': self.unified_experiment._get_experiment().hdf5_path,
            'source_hash': self._compute_source_hash(),
            'created_at': pd.Timestamp.now().isoformat(),
            'feature_count': len(final_df.columns) - 1
        }
    
    # ä¿å­˜
    final_df.to_parquet(output_path, compression='zstd', index=False)
    logger.info(f"âœ… å·²ä¿å­˜åˆ° {output_path}")
```

---

### æ¨¡å— 2ï¼šExecutor æ‰©å±•ï¼ˆæ”¯æŒåˆå§‹ä¸Šä¸‹æ–‡ï¼‰

**æ–‡ä»¶**ï¼š`infra/features_v2/core/executor.py`

```python
class Executor:
    def __init__(
        self,
        compute_graph: ComputeGraph,
        data_loaders: Optional[Dict[str, Callable]] = None,
        extractor_registry: Optional[Dict[str, Any]] = None,
    ):
        self.graph = compute_graph
        self.data_loaders = data_loaders or {}
        self.extractor_registry = extractor_registry or {}
    
    def execute(
        self, 
        initial_context: Optional[ExecutionContext] = None  # æ–°å¢å‚æ•°
    ) -> ExecutionContext:
        """
        æ‰§è¡Œè®¡ç®—å›¾
        
        Args:
            initial_context: é¢„å¡«å……çš„ä¸Šä¸‹æ–‡ï¼ˆç”¨äºç¼“å­˜ç‰¹å¾ï¼‰
        """
        context = initial_context or ExecutionContext()
        
        # æ‹“æ‰‘æ’åº
        execution_order = self.graph.topological_sort()
        
        logger.info(f"å¼€å§‹æ‰§è¡Œè®¡ç®—å›¾ï¼Œå…± {len(execution_order)} ä¸ªèŠ‚ç‚¹")
        
        # é€ä¸ªæ‰§è¡ŒèŠ‚ç‚¹
        for node_name in execution_order:
            self._execute_node(node_name, context)
        
        # è¾“å‡ºç»Ÿè®¡
        stats = context.get_statistics()
        logger.info(
            f"è®¡ç®—å›¾æ‰§è¡Œå®Œæˆï¼š{stats['total_features']} ä¸ªç‰¹å¾ï¼Œ"
            f"æ€»è€—æ—¶ {stats['total_time_ms']:.2f}ms"
        )
        
        return context
    
    def _execute_node(self, node_name: str, context: ExecutionContext):
        """æ‰§è¡Œå•ä¸ªèŠ‚ç‚¹"""
        # âœ… å¦‚æœå·²åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰ï¼Œç›´æ¥è·³è¿‡
        if context.has(node_name):
            context.cache_hits += 1
            logger.debug(f"è·³è¿‡å·²ç¼“å­˜èŠ‚ç‚¹: {node_name}")
            return
        
        context.cache_misses += 1
        
        # ... åŸæœ‰é€»è¾‘ï¼ˆæ•°æ®æºåŠ è½½ã€ç‰¹å¾è®¡ç®—ï¼‰
        # ï¼ˆä¿æŒä¸å˜ï¼‰
```

---

### æ¨¡å— 3ï¼šé…ç½® Schema æ‰©å±•

**æ–‡ä»¶**ï¼š`infra/features_v2/config/schema.py`

```python
from pydantic import BaseModel, Field

class FeatureConfig(BaseModel):
    """ç‰¹å¾é…ç½®ï¼ˆé¡¶å±‚ï¼‰"""
    version: str = "1.0"  # æ–°å¢ç‰ˆæœ¬å·
    name: str
    description: str = ""
    data_sources: List[DataSourceConfig] = []
    features: List[FeatureSpec] = []
    
    # ... å…¶ä»–å­—æ®µ
```

---

## ğŸ“Š å®æ–½è®¡åˆ’

| é˜¶æ®µ | ä»»åŠ¡ | æ–‡ä»¶ | å·¥ä½œé‡ |
|------|------|------|--------|
| **Phase 1** | æ ¸å¿ƒåŠŸèƒ½ | | **4-6 å°æ—¶** |
| 1.1 | FeatureSet æ„é€ å‡½æ•°æ‰©å±• | feature_set.py | 30 åˆ†é’Ÿ |
| 1.2 | from_config æ‰©å±• | feature_set.py | 20 åˆ†é’Ÿ |
| 1.3 | å¢é‡è®¡ç®—é€»è¾‘ | feature_set.py | 2 å°æ—¶ |
| 1.4 | ç¼“å­˜éªŒè¯ï¼ˆå“ˆå¸Œè®¡ç®—ï¼‰ | feature_set.py | 1 å°æ—¶ |
| 1.5 | Executor åˆå§‹ä¸Šä¸‹æ–‡æ”¯æŒ | executor.py | 30 åˆ†é’Ÿ |
| 1.6 | é…ç½® Schema ç‰ˆæœ¬å· | schema.py | 15 åˆ†é’Ÿ |
| **Phase 2** | å›ºåŒ–åŠŸèƒ½ | | **3-4 å°æ—¶** |
| 2.1 | save_as_config å®ç° | feature_set.py | 2 å°æ—¶ |
| 2.2 | to_parquet å¢é‡åˆå¹¶ | feature_set.py | 1 å°æ—¶ |
| 2.3 | å…ƒæ•°æ®è‡ªåŠ¨å†™å…¥ | feature_set.py | 30 åˆ†é’Ÿ |
| **Phase 3** | æµ‹è¯• & æ–‡æ¡£ | | **2-3 å°æ—¶** |
| 3.1 | å•å…ƒæµ‹è¯•ï¼ˆç¼“å­˜ã€åˆå¹¶ï¼‰ | tests/ | 1.5 å°æ—¶ |
| 3.2 | é›†æˆæµ‹è¯•ï¼ˆå®Œæ•´æµç¨‹ï¼‰ | tests/ | 1 å°æ—¶ |
| 3.3 | æ›´æ–°æ–‡æ¡£ + Demo | CLAUDE.md, notebook | 1 å°æ—¶ |
| **æ€»è®¡** | | | **9-13 å°æ—¶** |

---

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹ï¼ˆæœ€ç»ˆæ•ˆæœï¼‰

### åœºæ™¯ 1ï¼šé¦–æ¬¡æ¢ç´¢
```python
from infra.features_v2 import FeatureSet
from infra.catalog import quick_start

manager = quick_start()
exp = manager.get_experiment(chip_id="#20250804008", device_id="3")

# æ„å»ºåŸºç¡€ç‰¹å¾
features = FeatureSet(unified_experiment=exp)
features.add('gm_max', extractor='transfer.gm_max', input='transfer')
features.add('Von', extractor='transfer.Von', input='transfer')
features.add('absI_max', extractor='transfer.absI_max', input='transfer')

result = features.compute()  # 82 åˆ†é’Ÿ â±ï¸

# ğŸ’¾ å›ºåŒ–é…ç½® + æ•°æ®
info = features.save_as_config(
    config_name='my_base_features',
    save_parquet=True,
    config_dir='user',  # ä¿å­˜åˆ° ~/.my_features/
    description="æˆ‘çš„åŸºç¡€ç‰¹å¾é›†åˆ"
)
print(f"âœ“ é…ç½®: {info['config_file']}")
print(f"âœ“ æ•°æ®: {info['parquet_file']}")
```

### åœºæ™¯ 2ï¼šå¢é‡æ‰©å±•ï¼ˆç¬¬äºŒå¤©ï¼‰
```python
# åŠ è½½å·²å›ºåŒ–çš„ç‰¹å¾
features_v2 = FeatureSet.from_config(
    '~/.my_features/my_base_features.yaml',
    unified_experiment=exp
)

# æ·»åŠ æ´¾ç”Ÿç‰¹å¾
features_v2.add(
    'gm_normalized',
    func=lambda gm: (gm - gm.mean()) / gm.std(),
    input='gm_max'  # âœ… ä» Parquet ç¼“å­˜è¯»å–
)
features_v2.add(
    'gm_to_current_ratio',
    func=lambda gm, i: gm / (i + 1e-10),
    input=['gm_max', 'absI_max']  # âœ… éƒ½ä»ç¼“å­˜è¯»å–
)

result_v2 = features_v2.compute()
# âœ… gm_max, Von, absI_max ä» Parquet è¯»å–ï¼ˆ<1ç§’ï¼‰
# âš™ï¸ åªè®¡ç®— gm_normalized, gm_to_current_ratioï¼ˆ~1ç§’ï¼‰
# æ€»è€—æ—¶ï¼š~2ç§’ vs 82åˆ†é’Ÿ ğŸš€

# ğŸ’¾ å¢é‡ä¿å­˜ï¼ˆåˆå¹¶åˆ°åŸé…ç½®ï¼‰
info = features_v2.save_as_config(
    'my_base_features',
    append=True,  # âœ… æ™ºèƒ½åˆå¹¶
    save_parquet=True
)
print(f"âœ“ æ–°å¢ç‰¹å¾: {info['features_added']}")  
# ['gm_normalized', 'gm_to_current_ratio']
print(f"âœ“ é…ç½®ç‰ˆæœ¬: {info['config_version']}")  # 1.1
```

### åœºæ™¯ 3ï¼šç»§ç»­æ‰©å±•ï¼ˆç¬¬ä¸‰å¤©ï¼‰
```python
# å†æ¬¡åŠ è½½ï¼ˆåŒ…å«æ‰€æœ‰å†å²ç‰¹å¾ï¼‰
features_v3 = FeatureSet.from_config(
    '~/.my_features/my_base_features.yaml',
    unified_experiment=exp
)

# åŸºäºå·²æœ‰ç‰¹å¾ç»§ç»­æ‰©å±•
features_v3.add(
    'gm_ratio_smooth',
    func=lambda ratio: np.convolve(ratio, np.ones(10)/10, mode='same'),
    input='gm_to_current_ratio'  # âœ… ä»ç¼“å­˜è¯»å–
)

result_v3 = features_v3.compute()
# âœ… å‰ 5 ä¸ªç‰¹å¾å…¨éƒ¨ä»ç¼“å­˜è¯»å–
# âš™ï¸ åªè®¡ç®— gm_ratio_smooth
# æ€»è€—æ—¶ï¼š~1ç§’

# ğŸ’¾ å†æ¬¡å¢é‡ä¿å­˜
features_v3.save_as_config('my_base_features', append=True, save_parquet=True)
# é…ç½®ç‰ˆæœ¬ï¼š1.2
```

### åœºæ™¯ 4ï¼šç¼“å­˜è‡ªåŠ¨å¤±æ•ˆï¼ˆé‡æ–°è·‘å®éªŒï¼‰
```python
# å‡è®¾ä½ é‡æ–°è·‘äº†å®éªŒï¼ŒHDF5 æ–‡ä»¶æ›´æ–°äº†

features_v4 = FeatureSet.from_config(
    '~/.my_features/my_base_features.yaml',
    unified_experiment=exp
)

result_v4 = features_v4.compute()
# âš ï¸ æ£€æµ‹åˆ°æºæ–‡ä»¶å“ˆå¸Œæ”¹å˜ï¼Œç¼“å­˜å¤±æ•ˆ
# âš™ï¸ é‡æ–°è®¡ç®—æ‰€æœ‰ç‰¹å¾ï¼ˆ82åˆ†é’Ÿï¼‰

# ğŸ’¾ è‡ªåŠ¨ä¿å­˜æ–°ç¼“å­˜
features_v4.save_as_config('my_base_features', save_parquet=True)
# æ–° Parquet æ–‡ä»¶å¸¦æœ‰æ›´æ–°çš„ source_hash
```

---

## âœ… æ–¹æ¡ˆä¼˜åŠ¿

1. **å¢é‡å¼å·¥ä½œæµ**ï¼šæ¢ç´¢ â†’ å›ºåŒ– â†’ æ‰©å±• â†’ å†å›ºåŒ–
2. **æ€§èƒ½æå‡å·¨å¤§**ï¼š82åˆ†é’Ÿ â†’ ç§’çº§ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
3. **æ™ºèƒ½ç¼“å­˜ç®¡ç†**ï¼šè‡ªåŠ¨éªŒè¯ã€è‡ªåŠ¨å¤±æ•ˆ
4. **é…ç½®ç‰ˆæœ¬åŒ–**ï¼šæ”¯æŒå†å²è¿½æº¯
5. **çµæ´»é…ç½®ä¿å­˜**ï¼šä¸ªäºº + å…¨å±€ä¸¤ç§æ¨¡å¼
6. **æ— ç¼é›†æˆ**ï¼šä¸ç°æœ‰ CLI æ‰¹é‡æå–å…±äº«ç¼“å­˜

---

## ğŸš€ å¼€å§‹å®æ–½ï¼Ÿ

ç¡®è®¤åæˆ‘å°†æŒ‰ä»¥ä¸‹é¡ºåºå®æ–½ï¼š
1. **Phase 1**ï¼šæ ¸å¿ƒå¢é‡è®¡ç®—é€»è¾‘ï¼ˆå¿…éœ€ï¼‰
2. **Phase 2**ï¼šé…ç½®å›ºåŒ–åŠŸèƒ½ï¼ˆå®Œæ•´ä½“éªŒï¼‰
3. **Phase 3**ï¼šæµ‹è¯•å’Œæ–‡æ¡£ï¼ˆè´¨é‡ä¿è¯ï¼‰

é¢„è®¡æ€»å·¥ä½œé‡ï¼š**9-13 å°æ—¶**ï¼ˆ1-2 å¤©ï¼‰

å‡†å¤‡å¥½å¼€å§‹äº†å—ï¼ŸğŸ‰

---

## âœ… å®æ–½å®ŒæˆæŠ¥å‘Šï¼ˆ2025-10-31ï¼‰

### å®æ–½æ€»ç»“

**çŠ¶æ€**ï¼šâœ… **å…¨éƒ¨å®Œæˆ**

**å®é™…å·¥ä½œé‡**ï¼šçº¦ 3-4 å°æ—¶ï¼ˆæ¯”é¢„æœŸå¿«ï¼Œå› ä¸ºä»£ç ç»“æ„æ¸…æ™°ï¼‰

### å·²å®ç°åŠŸèƒ½

#### Phase 1ï¼šæ ¸å¿ƒåŸºç¡€è®¾æ–½ âœ…
1. **Schema æ‰©å±•** (`config/schema.py`)
   - âœ… æ·»åŠ  `name` å’Œ `config_version` å­—æ®µåˆ° `FeatureConfig`
   - âœ… æ”¯æŒé…ç½®ç‰ˆæœ¬è¿½è¸ª

2. **FeatureSet æ„é€ å‡½æ•°æ‰©å±•** (`core/feature_set.py`)
   - âœ… æ–°å¢ `unified_experiment`ã€`config_name`ã€`config_version` å‚æ•°
   - âœ… è‡ªåŠ¨æå–åº•å±‚ `experiment` å¯¹è±¡

3. **from_config æ–¹æ³•æ‰©å±•** (`config/parser.py`)
   - âœ… æ”¯æŒ `unified_experiment` å‚æ•°
   - âœ… è‡ªåŠ¨ä»æ–‡ä»¶åæå–é…ç½®åç§°

4. **Executor åˆå§‹ä¸Šä¸‹æ–‡æ”¯æŒ** (`core/executor.py`)
   - âœ… `execute()` æ–¹æ³•æ–°å¢ `initial_context` å‚æ•°
   - âœ… æ”¯æŒé¢„å¡«å……ç¼“å­˜ç‰¹å¾

5. **ç¼“å­˜éªŒè¯æœºåˆ¶** (`core/feature_set.py`)
   - âœ… `_compute_source_hash()`ï¼šè½»é‡çº§å“ˆå¸Œè®¡ç®—ï¼ˆåŸºäº mtime + sizeï¼‰
   - âœ… `_validate_cache()`ï¼šç¼“å­˜éªŒè¯ï¼ˆå«ä¸¥æ ¼æ¨¡å¼ï¼‰
   - âœ… å‘åå…¼å®¹æ—§ç¼“å­˜

6. **å¢é‡è®¡ç®—é€»è¾‘** (`core/feature_set.py: compute()`)
   - âœ… è‡ªåŠ¨ä» Parquet ç¼“å­˜åŠ è½½ç‰¹å¾
   - âœ… ç¼“å­˜éªŒè¯ï¼ˆæºæ–‡ä»¶å“ˆå¸ŒåŒ¹é…ï¼‰
   - âœ… åªè®¡ç®—ç¼ºå¤±ç‰¹å¾
   - âœ… ç¼“å­˜ç»Ÿè®¡ï¼ˆå‘½ä¸­ç‡ã€è€—æ—¶ï¼‰

#### Phase 2ï¼šé…ç½®å›ºåŒ–åŠŸèƒ½ âœ…
1. **to_parquet å¢å¼º** (`core/feature_set.py`)
   - âœ… `merge_existing` å‚æ•°ï¼šå¢é‡åˆå¹¶å·²æœ‰æ–‡ä»¶
   - âœ… `save_metadata` å‚æ•°ï¼šä¿å­˜ç¼“å­˜éªŒè¯å…ƒæ•°æ®
   - âœ… è¡Œæ•°ä¸€è‡´æ€§éªŒè¯
   - âœ… è‡ªåŠ¨æ›´æ–° `feature_count` å’Œ `updated_at`

2. **save_as_config æ–¹æ³•** (`core/feature_set.py`) **âœ¨ å…¨æ–°åŠŸèƒ½**
   - âœ… å°†ç‰¹å¾é›†å›ºåŒ–ä¸º YAML é…ç½®
   - âœ… æ™ºèƒ½é…ç½®åˆå¹¶ï¼ˆ`append=True` æ—¶å»é‡ã€ç‰ˆæœ¬é€’å¢ï¼‰
   - âœ… æ”¯æŒ `user` / `global` / è‡ªå®šä¹‰è·¯å¾„
   - âœ… Lambda å‡½æ•°åºåˆ—åŒ–
   - âœ… è‡ªåŠ¨ä¿å­˜ Parquet æ•°æ®

#### Phase 3ï¼šæµ‹è¯•ä¸æ–‡æ¡£ âœ…
1. **ç¤ºä¾‹è„šæœ¬**
   - âœ… `examples/incremental_workflow_demo.py`ï¼šå®Œæ•´ä¸‰é˜¶æ®µæ¼”ç¤º
   - âœ… åŒ…å«é¦–æ¬¡æ¢ç´¢ã€å¢é‡æ‰©å±•ã€ç¼“å­˜éªŒè¯

2. **å•å…ƒæµ‹è¯•**
   - âœ… `tests/test_incremental_compute.py`ï¼šæ ¸å¿ƒåŠŸèƒ½æµ‹è¯•
   - âœ… æµ‹è¯•è¦†ç›–ï¼šæ„é€ ã€ç¼“å­˜éªŒè¯ã€Parquet æ“ä½œã€é…ç½®å›ºåŒ–

3. **æ–‡æ¡£æ›´æ–°**
   - âœ… `CLAUDE.md`ï¼šå®Œæ•´çš„ API æ–‡æ¡£å’Œä½¿ç”¨ç¤ºä¾‹
   - âœ… å¢é‡å¼å·¥ä½œæµä¸‰é˜¶æ®µç¤ºä¾‹
   - âœ… æ ‡æ³¨ç‰ˆæœ¬ï¼šv2.1.0ï¼ˆå¢é‡å¼ç‰¹å¾å·¥ç¨‹ï¼‰

### æ ¸å¿ƒæ”¹è¿›ç‚¹

#### 1. æ€§èƒ½æå‡
- **82åˆ†é’Ÿ â†’ 2ç§’**ï¼ˆç¼“å­˜å‘½ä¸­æ—¶ï¼‰
- ç¼“å­˜å‘½ä¸­ç‡å¯è¾¾ 100%ï¼ˆå·²æœ‰ç‰¹å¾ï¼‰
- è½»é‡çº§å“ˆå¸ŒéªŒè¯ï¼ˆé¿å…è¯»å–å®Œæ•´æ–‡ä»¶ï¼‰

#### 2. ç”¨æˆ·ä½“éªŒ
- æ¢ç´¢ â†’ å›ºåŒ– â†’ æ‰©å±•çš„è‡ªç„¶å·¥ä½œæµ
- è‡ªåŠ¨ç¼“å­˜ç®¡ç†ï¼ˆæ— éœ€æ‰‹åŠ¨æ“ä½œï¼‰
- æ™ºèƒ½é…ç½®åˆå¹¶ï¼ˆé¿å…ç‰¹å¾ä¸¢å¤±ï¼‰

#### 3. æ•°æ®å®‰å…¨
- è‡ªåŠ¨ç¼“å­˜å¤±æ•ˆæ£€æµ‹
- é…ç½®ç‰ˆæœ¬è¿½è¸ª
- è¡Œæ•°ä¸€è‡´æ€§éªŒè¯

### ä»£ç è´¨é‡

**ä¿®æ”¹æ–‡ä»¶åˆ—è¡¨**ï¼š
1. `infra/features_v2/config/schema.py` - Schema æ‰©å±•
2. `infra/features_v2/core/feature_set.py` - æ ¸å¿ƒåŠŸèƒ½ï¼ˆ~250 è¡Œæ–°å¢ï¼‰
3. `infra/features_v2/core/executor.py` - åˆå§‹ä¸Šä¸‹æ–‡æ”¯æŒ
4. `infra/features_v2/config/parser.py` - é…ç½®è§£æå™¨æ‰©å±•
5. `infra/features_v2/CLAUDE.md` - æ–‡æ¡£æ›´æ–°

**æ–°å¢æ–‡ä»¶**ï¼š
1. `infra/features_v2/examples/incremental_workflow_demo.py` - æ¼”ç¤ºè„šæœ¬
2. `infra/features_v2/tests/test_incremental_compute.py` - å•å…ƒæµ‹è¯•

**æ€»ä»£ç è¡Œæ•°**ï¼š~600 è¡Œï¼ˆåŒ…å«æ³¨é‡Šå’Œæ–‡æ¡£ï¼‰

### ä½¿ç”¨ç¤ºä¾‹

```python
from infra.features_v2 import FeatureSet
from infra.catalog import quick_start

# ç¬¬ä¸€å¤©ï¼šå®šä¹‰åŸºç¡€ç‰¹å¾
manager = quick_start()
exp = manager.get_experiment(chip_id="#20250804008", device_id="3")

features = FeatureSet(unified_experiment=exp, config_name='my_features')
features.add('gm_max', extractor='transfer.gm_max', input='transfer')
features.add('Von', extractor='transfer.Von', input='transfer')

result = features.compute()  # 82 åˆ†é’Ÿ
features.save_as_config('my_features', save_parquet=True, config_dir='user')

# ç¬¬äºŒå¤©ï¼šå¢é‡æ‰©å±•
features_v2 = FeatureSet.from_config('~/.my_features/my_features.yaml', unified_experiment=exp)
features_v2.add('gm_norm', func=lambda gm: (gm - gm.mean())/gm.std(), input='gm_max')

result_v2 = features_v2.compute()  # ~2 ç§’ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
features_v2.save_as_config('my_features', append=True, save_parquet=True)
```

### æ½œåœ¨æ”¹è¿›ï¼ˆæœªæ¥ï¼‰

1. **Lambda å‡½æ•°åºåˆ—åŒ–æ”¹è¿›**
   - å½“å‰ï¼šä½¿ç”¨ `inspect.getsource()`
   - å»ºè®®ï¼šä½¿ç”¨ `dill` åº“æˆ– AST è§£æ

2. **é…ç½®å†²çªå¤„ç†ç­–ç•¥**
   - å½“å‰ï¼šç›´æ¥è¦†ç›–æˆ–è·³è¿‡
   - å»ºè®®ï¼šæ”¯æŒç‰ˆæœ¬åŒ–ï¼ˆ`feature_v2`ï¼‰

3. **å¹¶å‘å®‰å…¨**
   - å½“å‰ï¼šæ— æ–‡ä»¶é”
   - å»ºè®®ï¼šæ·»åŠ  `fcntl` æ–‡ä»¶é”

4. **ç¼“å­˜å¤±æ•ˆæ‰©å±•**
   - å½“å‰ï¼šä»…æ£€æµ‹æ–‡ä»¶å˜åŒ–
   - å»ºè®®ï¼šæ£€æµ‹æå–å™¨ä»£ç å˜åŒ–ï¼ˆé€šè¿‡ç‰ˆæœ¬å·ï¼‰

### æµ‹è¯•å»ºè®®

è¿è¡Œæ¼”ç¤ºè„šæœ¬ï¼š
```bash
cd /home/lidonghaowsl/develop/Minitest-OECT-dataprocessing
conda run --live-stream --name mlpytorch python infra/features_v2/examples/incremental_workflow_demo.py
```

è¿è¡Œå•å…ƒæµ‹è¯•ï¼š
```bash
conda run --live-stream --name mlpytorch pytest infra/features_v2/tests/test_incremental_compute.py -v
```

---

## ğŸ‰ ç»“è®º

å¢é‡å¼ç‰¹å¾å·¥ç¨‹ç³»ç»Ÿå·²**å…¨é¢å®Œæˆ**ï¼Œæä¾›äº†ï¼š
- âœ… 82åˆ†é’Ÿ â†’ 2ç§’çš„æ€§èƒ½æå‡
- âœ… æ¢ç´¢ â†’ å›ºåŒ– â†’ æ‰©å±•çš„å®Œæ•´å·¥ä½œæµ
- âœ… è‡ªåŠ¨ç¼“å­˜ç®¡ç†å’ŒéªŒè¯
- âœ… æ™ºèƒ½é…ç½®åˆå¹¶
- âœ… å®Œæ•´çš„æ–‡æ¡£å’Œç¤ºä¾‹

ç³»ç»Ÿå¯ç«‹å³æŠ•å…¥ä½¿ç”¨ï¼Œæ”¯æŒæ—¥å¸¸ç‰¹å¾å·¥ç¨‹è¿­ä»£å·¥ä½œï¼ğŸš€